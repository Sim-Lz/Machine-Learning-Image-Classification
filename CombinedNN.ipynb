{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "import pytesseract\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "dataset_dir = '/Users/simone/Desktop/Luiss /*Machine Learning/Reply project/Document-classification/Tobacco3482-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personalize the dataset for PyTorch compatibility\n",
    "class TobaccoDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.classes = [d for d in os.listdir(img_dir) if os.path.isdir(os.path.join(img_dir, d))]\n",
    "        self.files = []\n",
    "        for label in self.classes:\n",
    "            class_dir = os.path.join(img_dir, label)\n",
    "            class_files = [(os.path.join(class_dir, file), label) for file in os.listdir(class_dir) if file.endswith('.jpg')]\n",
    "            self.files.extend(class_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.classes.index(label), img_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset and define the train/val split\n",
    "dataset = TobaccoDataset(dataset_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For next section, be sure to have installed tesseract from homebrew: `brew install tesseract`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from images using TF-IDF vectorizer\n",
    "def extract_text_from_image(image_path):\n",
    "    return pytesseract.image_to_string(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [extract_text_from_image(img_path) for _, _, img_path in train_dataset]\n",
    "val_texts = [extract_text_from_image(img_path) for _, _, img_path in val_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "train_text_features = vectorizer.fit_transform(train_texts).toarray()\n",
    "val_text_features = vectorizer.transform(val_texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_features = torch.tensor(train_text_features, dtype=torch.float32)\n",
    "val_text_features = torch.tensor(val_text_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CombinedNN model\n",
    "class CombinedNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super(CombinedNN, self).__init__()\n",
    "\n",
    "        # VGG16 to extract visual features\n",
    "        self.vgg16 = models.vgg16(weights=\"IMAGENET1K_V1\")\n",
    "        self.vgg16.classifier = nn.Sequential(*list(self.vgg16.classifier.children())[:-1])  # Rimuovere l'ultimo livello\n",
    "\n",
    "        # NN for visual features\n",
    "        self.visual_fc1 = nn.Linear(4096, 512)\n",
    "        self.visual_dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.visual_fc2 = nn.Linear(512, 256)\n",
    "        self.visual_dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.visual_fc3 = nn.Linear(256, 128)\n",
    "        self.visual_dropout3 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # NN for textual features\n",
    "        self.text_fc1 = nn.Linear(1000, 512)\n",
    "        self.text_dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.text_fc2 = nn.Linear(512, 256)\n",
    "        self.text_dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.text_fc3 = nn.Linear(256, 128)\n",
    "        self.text_dropout3 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # Concatenation and final layer\n",
    "        self.final_fc = nn.Linear(128 + 128, num_classes)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # Visual features\n",
    "        x1 = self.vgg16(images)\n",
    "        x1 = torch.relu(self.visual_fc1(x1))\n",
    "        x1 = self.visual_dropout1(x1)\n",
    "        x1 = torch.relu(self.visual_fc2(x1))\n",
    "        x1 = self.visual_dropout2(x1)\n",
    "        x1 = torch.relu(self.visual_fc3(x1))\n",
    "        x1 = self.visual_dropout3(x1)\n",
    "\n",
    "        # Textual features\n",
    "        x2 = torch.relu(self.text_fc1(texts))\n",
    "        x2 = self.text_dropout1(x2)\n",
    "        x2 = torch.relu(self.text_fc2(x2))\n",
    "        x2 = self.text_dropout2(x2)\n",
    "        x2 = torch.relu(self.text_fc3(x2))\n",
    "        x2 = self.text_dropout3(x2)\n",
    "\n",
    "        # Concatenation and final layer\n",
    "        combined = torch.cat((x1, x2), dim=1)\n",
    "        output = self.final_fc(combined)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Model creation\n",
    "model = CombinedNN(num_classes=num_classes)\n",
    "\n",
    "# Model Compiling\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom summary function. \n",
    "# Pytorch summary function does not work for CombinedNN since our model takes multiple inputs\n",
    "def print_model_summary(model, input_shapes):\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"output_shape\"] = list(output.size())\n",
    "            summary[m_key][\"trainable\"] = any(p.requires_grad for p in module.parameters())\n",
    "            params = 0\n",
    "            for p in module.parameters():\n",
    "                params += torch.prod(torch.tensor(p.size()))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "            and not (module == model)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    import numpy as np\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # Create inputs with the correct shape\n",
    "    inputs = [torch.rand(2, *in_shape).to(device) for in_shape in input_shapes]\n",
    "    model(*inputs)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"        Layer (type)               Output Shape         Param #\")\n",
    "    print(\"================================================================\")\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        line_new = \"{:>25}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"]:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        print(line_new)\n",
    "\n",
    "    print(\"================================================================\")\n",
    "    print(\"Total params: {0:,}\".format(total_params))\n",
    "    print(\"Trainable params: {0:,}\".format(trainable_params))\n",
    "    print(\"Non-trainable params: {0:,}\".format(total_params - trainable_params))\n",
    "    print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "print_model_summary(model, [(3, 224, 224), (1000,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy inputs\n",
    "dummy_images = torch.randn(1, 3, 224, 224).to(device)\n",
    "dummy_texts = torch.randn(1, 1000).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CombinedNN_architecture.png'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the model\n",
    "from torchviz import make_dot\n",
    "output = model(dummy_images, dummy_texts)\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.render(\"CombinedNN_architecture\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train_model(model, train_loader, val_loader, train_text_features, val_text_features, criterion, optimizer, num_epochs=20, patience=3):\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    accuracy_list = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    " \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (images, labels, _) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            texts = train_text_features[i * images.size(0):(i + 1) * images.size(0)].to(device)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images, texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels, _) in enumerate(val_loader):\n",
    "                texts = val_text_features[i * images.size(0):(i + 1) * images.size(0)].to(device)\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(images, texts)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        accuracy_list.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%, '\n",
    "              f'Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve == patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    return train_loss_list, val_loss_list, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training \n",
    "train_loss, validation_loss, accuracy = train_model(model, train_loader, val_loader, train_text_features, val_text_features, criterion, optimizer, num_epochs=30, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"CombinedNN.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss, validation loss and accuracy\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(train_loss)))\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(accuracy, label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(accuracy)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "Contrary to our predictions, the model does not yield optimal results. We decided not to investigate the performances on the test set, but we left the code anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "test_dir = \"/Users/simone/Desktop/Luiss /*Machine Learning/Reply project/Document-classification/Reply_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms for the test set\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for the test set\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model\n",
    "model_load_path = \"CombinedNN.pth\"\n",
    "model = CombinedNN(num_classes=len(test_dataset.classes)).to(device)\n",
    "model.load_state_dict(torch.load(model_load_path))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(f\"Model loaded from {model_load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "class_report = classification_report(y_true, y_pred, target_names=test_dataset.classes)\n",
    " \n",
    "print(f\"Accuracy: {accuracy} \\n \\nF1 Score: {f1}\\n \\nPrecision: {precision}\\n \\nConfusion Matrix:\\n {conf_matrix}\\n \\nClassification Report:\\n {class_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(test_dataset.classes))\n",
    "plt.xticks(tick_marks, test_dataset.classes, rotation=45)\n",
    "plt.yticks(tick_marks, test_dataset.classes)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
